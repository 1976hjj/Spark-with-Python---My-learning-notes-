{"cells":[{"cell_type":"code","source":["spark.sql(\"show schemas\").show()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["dfStatic = spark.read\\\n.format(\"csv\")\\\n.option(\"header\",\"true\")\\\n.option(\"inferSchema\",\"true\")\\\n.load(\"/FileStore/tables/retailDate.csv\")\n\nstaticSchema = dfStatic.schema\nstaticSchema"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#1. create stream DF\ndfStream = spark.readStream\\\n.option(\"masFilesPerTrigger\", 1)\\\n.format(\"csv\")\\\n.option(\"header\",\"true\")\\\n.schema(staticSchema)\\\n.load(\"/FileStore/tables/*.csv\")\\"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dfStream.isStreaming"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#2. some code to process straming data\nfrom pyspark.sql.functions import col, window\ndfNew = dfStream.selectExpr(\"CustomerID\"\n                           , \"(UnitPrice * Quantity) as total_cost\"\n                           ,\"InvoiceDate\")\\\n                           .groupBy( col(\"CustomerID\"), window(col(\"InvoiceDate\"),\"1 day\"))\\\n                            .sum(\"total_cost\")\n                           "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#3. start writing stream to memory \ndfNew.writeStream\\\n.format(\"memory\")\\\n.queryName(\"test\")\\\n.outputMode(\"complete\")\\\n.start()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#4. read from memory\nspark.sql(\"select * from test\").show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["dfNew.writeStream\\\n.format(\"console\")\\\n.queryName(\"test_2\")\\\n.outputMode(\"complete\")\\\n.start()\n\n\n#Neither of these streaming methods should be used in production \n"],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"sparkStreaming","notebookId":863376660785174},"nbformat":4,"nbformat_minor":0}
