{"cells":[{"cell_type":"code","source":["df = spark.createDataFrame([[\"A\",10],[\"B\",20]])\ndf.show()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#None = null\ndf_2 = spark.createDataFrame([{\"ID\":\"A\",\"NO\":10},{\"ID\":\"B\",\"NO\":20},{\"ID\":None,\"NO\":30}])\ndf_2.show()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#eqNullSafe\nfrom pyspark.sql.functions import col\ndf_2.where(col('ID').eqNullSafe(\"hello\")).show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#lit\nfrom pyspark.sql.functions import lit\ndf_2.select(lit(5),lit(\"sda\")).show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Converting to Spark Types\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Working with Booleans\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#Working with Numbers\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Working with Strings\n\nfrom pyspark.sql.functions import initcap, lower\ndf_2.select(initcap(\"ID\")).show()\ndf_2.select(lower(col(\"ID\"))).show()\ndf_2.withColumn(\"newCol\", lower(col(\"ID\"))).select(\"newCol\").show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#Regular Expressions\nfrom pyspark.sql.functions import regexp_replace\ndf_2.select(regexp_replace(col(\"ID\"),\"A\",\"AAA\")).show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#Working with Dates and Timestamps\nfrom pyspark.sql.functions import current_date, current_timestamp\ndf_3 = spark.range(5).withColumn(\"today\",current_date()).withColumn(\"now\", current_timestamp())\ndf_3.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql.functions import date_add , date_sub, datediff, months_between, to_date\ndf_3.select(date_sub(col(\"today\"),5)).show()\ndf_3.select(to_date(lit(\"2017-12-01\")).alias(\"start\"), to_date(lit(\"2017-01-01\")).alias(\"end\")).select(months_between(col(\"start\"),col(\"end\"))).show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Working with Nulls in Data\n#coalesce\nfrom pyspark.sql.functions import coalesce \ndf_2.select(coalesce(col(\"NO\"),col(\"ID\"))).show()\n\ndf_2.select(coalesce(col(\"ID\"),col(\"NO\"))).show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#from pyspark.sql.functions import  nvl, nvl2 # nullif, ifnull, \ndf_2.createOrReplaceTempView(\"tmpTbl\")\nspark.sql(\"select nvl(ID,'AAA') from tmpTbl\").show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df_2.na.drop(\"any\").show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#Working with Complex Types\n"],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"dataframe_API","notebookId":2162637923774473},"nbformat":4,"nbformat_minor":0}
